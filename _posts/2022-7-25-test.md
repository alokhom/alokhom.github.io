Migrate from Cassandra in a vanilla STS to K8ssandra
----------------------------------------------------

Most users of k8ssandra have either started a new Cassandra cluster or have migrated from an existing Cassandra cluster. 

In containerized Cassandra clusters like the vanilla STS Cassandra cluster, there is no earlier backups made perhaps (besides disc snapsots) or rather Admins are stuck in a assumption that the k8ssandra migration is straight forward and flawless. 

The Risk is what if the migration to k8ssandra is not a success ? And therefore what is the fallback / migrating plan from a STS cluster point of view. It involves risk of losing data. 

Can we have a backup/restore test on the vanilla STS cluster before we proceed with migration to k8ssandra cluster ?

There is no documented resources on the internet for backup and restore of vanilla STS cluster and an attempt was made with this solution and it kind of works. 
Repo: https://github.com/alokhom/cassandra-statefulset-to-k8ssandra-migration 
Request feedback and continuous improvement. 

To simulate the scenario let us make a STS cluster on gke. 
------------------------------------------------------------
- We have 3 pod vanilla STS cluster with 10 GB disc each.
- K8Demo is the cluster name
- We assume some clients old and new are using typical CASSANDRA varaibles to connect to the cluster via a common configMap that can also be used as a env variable on this manifest.
```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cassandra
  labels:
    app: cassandra
spec:
  serviceName: cassandra
  replicas: 3
  selector:
    matchLabels:
      app: cassandra
  template:
    metadata:
      labels:
        app: cassandra
    spec:
      terminationGracePeriodSeconds: 1800
      containers:
      - name: cassandra
        image: gcr.io/google-samples/cassandra:v13
        imagePullPolicy: Always
        ports:
        - containerPort: 7000
          name: intra-node
        - containerPort: 7001
          name: tls-intra-node
        - containerPort: 7199
          name: jmx
        - containerPort: 9042
          name: cql
        resources:
          limits:
            cpu: "500m"
            memory: 1Gi
          requests:
            cpu: "500m"
            memory: 1Gi
        securityContext:
          capabilities:
            add:
              - IPC_LOCK
        lifecycle:
          preStop:
            exec:
              command: 
              - /bin/sh
              - -c
              - nodetool drain
        env:
          - name: MAX_HEAP_SIZE
            value: 512M
          - name: HEAP_NEWSIZE
            value: 100M
          - name: CASSANDRA_SEEDS
            value: "cassandra-0.cassandra.default.svc.cluster.local"
          - name: CASSANDRA_CLUSTER_NAME
            value: "K8Demo"
          - name: CASSANDRA_DC
            value: "DC1-K8Demo"
          - name: CASSANDRA_RACK
            value: "Rack1-K8Demo"
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - /ready-probe.sh
          initialDelaySeconds: 15
          timeoutSeconds: 5
        # These volume mounts are persistent. They are like inline claims,
        # but not exactly because the names need to match exactly one of
        # the stateful pod volumes.
        volumeMounts:
        - name: cassandra-data
          mountPath: /cassandra_data
  volumeClaimTemplates:
  - metadata:
      name: cassandra-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: standard-rwo
      resources:
        requests:
          storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: cassandra
  name: cassandra
spec:
  ports:
  - port: 9042
  selector:
    app: cassandra
```

- In this yaml there is no scope of automated backup usign medusa, a popular cassandra backup solution.  ( Disc snapshots in google cloud are possible and has their own risks involved. ) 

Using medusa over a vanilla sts cassandra cluster - no downtime.
--------------------------------------------------------------
- Read more about Medusa here. https://github.com/thelastpickle/cassandra-medusa
- Medusa is also used in k8ssandra operator.
- Medusa is also offered as a Docker image. 
- You will need a s3 storage to backup and restore your vanilla sts cluster and configure your s3 bucket. For this demo we have selected the gcs setup from https://github.com/thelastpickle/cassandra-medusa/tree/master/docs. You could select your own s3 preference. We have used gcs s3 config here. Refer to the medusa.ini inside the ConfigMap to find this stub. Ensure the file name is medusa_gcp_key.json and use the bucket name that was created.
```
[storage]
storage_provider = google_storage
bucket_name = gcs_bucket_name
key_file = /etc/medusa/medusa_gcp_key.json
```
Using medusa in the sts vanilla cluster
---------------------------------------
<b>Note</b>: Submit all the changes of the sts in one go.
- The idea is to trigger daily backups using medusa via a CronJob.
- We will use the jolokia jvm agent for backing up here. 
- We will use a jolokia jvm agent in via the initContainer and volumeMount on the cassandra sts container.
- The jolokia-share volume helps provide the jolokia jar from the initContainer to the medusa container.
- Add the jolokia initContainer to the cassandra sts manifest
```
 initContainers:
   - name: install-jolokia-jvm-agent
     image: busybox
     command:
       - sh
       - '-c'
       - >-
         wget -O /usr/share/java/jolokia-jvm-1.6.2-agent.jar
         http://search.maven.org/remotecontent?filepath=org/jolokia/jolokia-jvm/1.6.2/jolokia-jvm-1.6.2-agent.jar
     resources: {}
     volumeMounts:
       - name: jolokia-share
         mountPath: /usr/share/java
```
- Add extra volumes on the cassandra sts manifest.
```
 volumes:
   - name: jolokia-share
     emptyDir: {}
   - name: server-config
     emptyDir: {}
   - name: cassandra-medusa
     configMap:
       name: scripts
       items:
         - key: medusa.ini
           path: medusa.ini
   - name: google-storage-s3-json
     secret:
       secretName: google-storage-s3-json
       defaultMode: 420
   - name: medusa-scripts
     configMap: 
       defaultMode: 0755
       name: scripts
       items:
         - key: get_cassandra_node_names.sh
           path: get_cassandra_node_names.sh
```
- Add volumeMounts on the cassandra container.
```
     volumeMounts:
       - name: medusa-scripts
         mountPath: /scripts/medusa-scripts/
```
- Add the environment variables to the sts cassandra container. Ensure the value for variable CASSANDRA_DC(if not there). 
```
             - name: CASSANDRA_DC
               value: K8Demo
             - name: CASSANDRA_ENDPOINT_SNITCH
               value: GossipingPropertyFileSnitch
             - name: JVM_EXTRA_OPTS
               value: -javaagent:/usr/share/java/jolokia-jvm-1.6.2-agent.jar=port=8778,host=localhost
```
- Add the medusa container block.
```
   - name: medusa
     image: docker.io/k8ssandra/medusa:0.12.2
     ports:
       - containerPort: 50051
         protocol: TCP
     env:
       - name: MEDUSA_MODE
         value: GRPC
     resources: {}
     volumeMounts:
       - name: server-config
         mountPath: /etc/cassandra
       - name: cassandra-medusa
         mountPath: /etc/medusa
       - name: cassandra-data
         mountPath: /var/lib/cassandra
       - name: google-storage-s3-json
         mountPath: /etc/medusa-secrets
     livenessProbe:
       exec:
         command:
           - /bin/grpc_health_probe
           - '-addr=:50051'
       initialDelaySeconds: 10
       timeoutSeconds: 1
       periodSeconds: 10
       successThreshold: 1
       failureThreshold: 3
     readinessProbe:
       exec:
         command:
           - /bin/grpc_health_probe
           - '-addr=:50051'
       initialDelaySeconds: 5
       timeoutSeconds: 1
       periodSeconds: 10
       successThreshold: 1
       failureThreshold: 3
     terminationMessagePath: /dev/termination-log
     terminationMessagePolicy: File
     imagePullPolicy: IfNotPresent
     securityContext: {}
```