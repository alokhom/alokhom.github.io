Migrate from Cassandra in a vanilla STS to K8ssandra
----------------------------------------------------
Most users of k8ssandra have either started a new Cassandra cluster or have migrated from an existing Cassandra cluster. 

In containerized Cassandra clusters like the vanilla STS Cassandra cluster, there is no backups made perhaps (besides disc snapsots) or rather Admins are stuck in a assumption that the migration to k8ssandra is straight forward and flawless. 

The Risk is what if the migration to k8ssandra is not a success ? And therefore what is the fallback / migrating plan from a STS cluster point of view. It involves risk of losing data. Can we have a backup/restore test on the vanilla STS cluster before we proceed with migration to k8ssandra cluster ?

There is no documented procedure on the internet for backup and restore of vanilla STS cluster and an attempt was made with this solution and it kind of works. Repo: https://github.com/alokhom/cassandra-statefulset-to-k8ssandra-migration 

To simulate the scenario let us make a STS cluster on gke. 
------------------------------------------------------------
- We have a 3 pod vanilla STS cluster with 10 GB disc each. K8Demo is the cluster name.
- We assume some clients old and new are using typical CASSANDRA varaibles to connect to the cluster.
- On gke, make a namespace cassandra and apply the vanilla cassandra sts manifest.
```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cassandra
  namespace: cassandra
  labels:
    app: cassandra
spec:
  serviceName: cassandra
  replicas: 3
  selector:
    matchLabels:
      app: cassandra
  template:
    metadata:
      labels:
        app: cassandra
    spec:
      terminationGracePeriodSeconds: 1800
      containers:
      - name: cassandra
        image: gcr.io/google-samples/cassandra:v13
        imagePullPolicy: Always
        ports:
        - containerPort: 7000
          name: intra-node
        - containerPort: 7001
          name: tls-intra-node
        - containerPort: 7199
          name: jmx
        - containerPort: 9042
          name: cql
        resources:
          limits:
            cpu: "500m"
            memory: 1Gi
          requests:
            cpu: "500m"
            memory: 1Gi
        securityContext:
          capabilities:
            add:
              - IPC_LOCK
        lifecycle:
          preStop:
            exec:
              command: 
              - /bin/sh
              - -c
              - nodetool drain
        env:
          - name: MAX_HEAP_SIZE
            value: 512M
          - name: HEAP_NEWSIZE
            value: 100M
          - name: CASSANDRA_SEEDS
            value: "cassandra-0.cassandra.default.svc.cluster.local"
          - name: CASSANDRA_CLUSTER_NAME
            value: "K8Demo"
          - name: CASSANDRA_DC
            value: "DC1-K8Demo"
          - name: CASSANDRA_RACK
            value: "Rack1-K8Demo"
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - /ready-probe.sh
          initialDelaySeconds: 15
          timeoutSeconds: 5
        # These volume mounts are persistent. They are like inline claims,
        # but not exactly because the names need to match exactly one of
        # the stateful pod volumes.
        volumeMounts:
        - name: cassandra-data
          mountPath: /cassandra_data
  volumeClaimTemplates:
  - metadata:
      name: cassandra-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: standard-rwo
      resources:
        requests:
          storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: cassandra
  name: cassandra
spec:
  ports:
  - port: 9042
  selector:
    app: cassandra
```
- In the above yaml there is no medusa support, a popular cassandra backup solution.  
- PV snapshots in google cloud are possible and has their own risks involved.

Using medusa backup on the vanilla sts cassandra cluster with zero downtime.
---------------------------------------------------------------------------
- Read more about Medusa here. https://github.com/thelastpickle/cassandra-medusa. Medusa is also used in k8ssandra operator.Medusa is also offered as a Docker image. 
- You will need a s3 storage bucket to backup and restore the vanilla cassandra STS cluster. For this demo we have selected the gcs setup from https://github.com/thelastpickle/cassandra-medusa/tree/master/docs. You could select your own s3 preference. Refer to the medusa.ini inside the ConfigMap referred below to find the storage stub. Ensure the file name is medusa_gcp_key.json and use the bucket name that was created.
```
[storage]
storage_provider = google_storage
bucket_name = gcs_bucket_name
key_file = /etc/medusa/medusa_gcp_key.json
```
Create and apply secret. 
-----------------------
```
kubectl create secret generic google-storage-s3-json -n <namespace-of-sts-cassandra> --from-file=./medusa_gcp_key.json
```
Modifying the vanilla cassandra STS yaml file.
----------------------------------------------
<b>Note</b>: Submit all the changes of the sts in one go.
- The idea is to trigger a backup/backups using Medusa via a CronJob.
- Edit the STS yaml file.
- We will use the jolokia jvm agent for backing up here via the initContainer and volumeMount on the cassandra sts container. Management API is not used here.
- The jolokia-share volume helps provide the jolokia jar from the initContainer to the medusa container.
- Add the jolokia initContainer code block to the cassandra sts yaml
```
 initContainers:
   - name: install-jolokia-jvm-agent
     image: busybox
     command:
       - sh
       - '-c'
       - >-
         wget -O /usr/share/java/jolokia-jvm-1.6.2-agent.jar
         http://search.maven.org/remotecontent?filepath=org/jolokia/jolokia-jvm/1.6.2/jolokia-jvm-1.6.2-agent.jar
     resources: {}
     volumeMounts:
       - name: jolokia-share
         mountPath: /usr/share/java
```
- Add extra volumes on the cassandra sts yaml.
```
 volumes:
   - name: jolokia-share
     emptyDir: {}
   - name: server-config
     emptyDir: {}
   - name: cassandra-medusa
     configMap:
       name: scripts
       items:
         - key: medusa.ini
           path: medusa.ini
   - name: google-storage-s3-json
     secret:
       secretName: google-storage-s3-json
       defaultMode: 420
   - name: medusa-scripts
     configMap: 
       defaultMode: 0755
       name: scripts
       items:
         - key: get_cassandra_node_names.sh
           path: get_cassandra_node_names.sh
```
- Modify/add volumeMounts on the cassandra container.
```
     volumeMounts:
       - name: medusa-scripts
         mountPath: /scripts/medusa-scripts/
```
- Modify/add the environment variables to the sts cassandra container. Ensure the value for variable CASSANDRA_DC(if not there). 
```
             - name: CASSANDRA_DC
               value: K8Demo
             - name: CASSANDRA_ENDPOINT_SNITCH
               value: GossipingPropertyFileSnitch
             - name: JVM_EXTRA_OPTS
               value: -javaagent:/usr/share/java/jolokia-jvm-1.6.2-agent.jar=port=8778,host=localhost
```
- Add the medusa container block to the sts manifest
```
   - name: medusa
     image: docker.io/k8ssandra/medusa:0.12.2
     ports:
       - containerPort: 50051
         protocol: TCP
     env:
       - name: MEDUSA_MODE
         value: GRPC
     resources: {}
     volumeMounts:
       - name: server-config
         mountPath: /etc/cassandra
       - name: cassandra-medusa
         mountPath: /etc/medusa
       - name: cassandra-data
         mountPath: /var/lib/cassandra
       - name: google-storage-s3-json
         mountPath: /etc/medusa-secrets
     livenessProbe:
       exec:
         command:
           - /bin/grpc_health_probe
           - '-addr=:50051'
       initialDelaySeconds: 10
       timeoutSeconds: 1
       periodSeconds: 10
       successThreshold: 1
       failureThreshold: 3
     readinessProbe:
       exec:
         command:
           - /bin/grpc_health_probe
           - '-addr=:50051'
       initialDelaySeconds: 5
       timeoutSeconds: 1
       periodSeconds: 10
       successThreshold: 1
       failureThreshold: 3
     terminationMessagePath: /dev/termination-log
     terminationMessagePolicy: File
     imagePullPolicy: IfNotPresent
     securityContext: {}
```
Create and apply the below configMap 
------------------------------------
`cassandra-backup.ConfigMap.yaml` to the same namespace as cassandra STS yaml, if not default namespace.It offers 
- kubectl cli (configure_k8s.sh)
- Discovering cassandra nodes names (ascertain_cassandra_nodes.sh and get_cassandra_node_names.sh)
- Copying the cassandra.yml file from the cassandra-0 node to the medusa container of all nodes. 
- Firing backup from a python file. (backup_with_medusa.sh and insert.py)
- The Cassandra Administrator is requested to add the right jmx password as follows in the file jmxremote.password
```
  jmx_user jmx123
```
- The Cassandra Administrator is requested to add the right jmx user as follows in the file jmxremote.access.
```
  jmx_user readwrite
```
Ensure the correct nodetool username as jmx_user and configure it in the medusa.ini
```
  nodetool_username = jmx_user
```
- Medusa needs a medusa ini file configuration to be used for backup. (medusa.ini). This ini file also has the storage configuration of s3 and credentials of nodetool password. The grpc should be enabled (enabled = 1). Kubernetes should be enabled and cassandra_url should point to http://127.0.0.1:8778/jolokia/ . Management API is turned off. (use_mgmt_api = 0) <b>Note</b>: Make changes to the medusa.ini block whereever it requires, e.g. file and path of nodetool_password_file_path.
```
kind: ConfigMap
apiVersion: v1
metadata:
  name: scripts
  # namespace: cassandra or <namespace-of-sts-cassandra>
data:
  configure_k8s.sh: |
    #!/bin/sh
    curl -sLO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" && chmod +x kubectl && mv ./kubectl /usr/bin/kubectl
    echo "kubectl installed"

  cassandra_backup.yaml: |
    apiVersion: cassandra.k8ssandra.io/v1alpha1
    kind: CassandraBackup
    metadata:
      name: medusa-daily-timestamp
      namespace: cassandra
    spec:
      backupType: "differential"
      name: medusa-daily-timestamp
      cassandraDatacenter: dc1

  get_cassandra_node_names.sh: |
    #!/bin/sh
    apt-get update > /dev/null
    apt-get install sudo -y > /dev/null
    apt-get install dnsutils -qq >/tmp/dns.out;
    if [ -f /tmp/cassandrahostlist ];then rm /tmp/cassandrahostlist; fi
    cat /tmp/cassandraIPlist | while read line;do dig -x $line +short | awk -F"." '{print $1}' >> /tmp/cassandrahostlist;done
    sleep 1;

  ascertain_cassandra_nodes.sh: |
    #!/bin/sh 
    # change namesapce to default if it is needed. Check which namespace the cassandra sts pods are running in the source cluster.
    kubectl exec -it cassandra-0 -n cassandra -c cassandra -- bash -c "[[ -f /secrets/jmxremote.password ]] && nodetool -h ::FFFF:127.0.0.1 -u jmx_user -pwf /secrets/jmxremote.password status | grep UN | cut -d ' ' -f3 > /tmp/cassandraIPlist";sleep 1;

    kubectl exec -it cassandra-0 -n cassandra -c cassandra -- bash -c "./scripts/medusa-scripts/get_cassandra_node_names.sh";sleep 1;
    kubectl exec -it cassandra-0 -n cassandra -c cassandra -- bash -c "cat /tmp/cassandrahostlist" | tee /tmp/hostlist;
    echo "nodes acertained";sleep 5;

  copy_yaml.sh: |
    #!/bin/sh

    # the idea was to copy the cassandra yaml from cassandra node to medusa container in cassandra nodes.  for example a 3 node cassandra. 

    # change namesapce to default if it is needed. the cassandra node should be available  in the namespace. 

    for node in $(cat /tmp/hostlist)
    do
      nodename="$(echo $node| sed 's/\r$//')"
      sleep 2
      kubectl exec pod/$nodename -n cassandra -c cassandra -- tar cf - /etc/cassandra/cassandra.yaml | kubectl exec -i pod/$nodename -n cassandra -c medusa -- bash -c 'tar xvf - -C /tmp && if [ -f /etc/cassandra/cassandra.yaml  ];then rm -f /etc/cassandra/cassandra.yaml; fi; cp /tmp/etc/cassandra/cassandra.yaml /etc/cassandra && ls /etc/cassandra && sleep 2';sleep 5;
    done


  backup_with_medusa.sh: |
    #!/bin/sh
    cp insert.py client_candidate.py
    sleep 1
    dateTime=backup-"$(date +"%m-%d-%Y-%H-%M-%S")"
    for node in $(cat /tmp/hostlist)
    do
      nodename="$(echo $node| sed 's/\r$//').cassandra"
      sleep 5
      echo "............ starting backup of $nodename ...."
      sleep 5
      cat client_candidate.py | sed -e s/localhost/"$nodename"/g  > client.py
      sleep 5
      python ./client.py "$dateTime"
      sleep 5
      echo "............ backup of $nodename complete...."
      sleep 5
    done
    sleep 20;

  insert.py: |
    import time
    from datetime import datetime
    import medusa_pb2
    import medusa_pb2_grpc
    backupName = sys.argv[1]

    import grpc
    import logging
    from grpc_health.v1 import health_pb2
    from grpc_health.v1 import health_pb2_grpc

    from medusa.service.grpc import medusa_pb2
    from medusa.service.grpc import medusa_pb2_grpc


    class Client:
        def __init__(self, target, channel_options=[]):
            self.channel = grpc.insecure_channel(target, options=channel_options)

        def health_check(self):
            try:
                health_stub = health_pb2_grpc.HealthStub(self.channel)
                request = health_pb2.HealthCheckRequest()
                return health_stub.Check(request)
            except grpc.RpcError as e:
                logging.error("Failed health check due to error: {}".format(e))
                return None

        def create_backup_stub(self, mode):
            stub = medusa_pb2_grpc.MedusaStub(self.channel)
            if mode == "differential":
                backup_mode = 0
            elif mode == "full":
                backup_mode = 1
            else:
                raise RuntimeError("{} is not a recognized backup mode".format(mode))
            return backup_mode, stub

        def async_backup(self, name, mode):
            try:
                backup_mode, stub = self.create_backup_stub(mode=mode)
                request = medusa_pb2.BackupRequest(name=name, mode=backup_mode)
                return stub.AsyncBackup(request)
            except grpc.RpcError as e:
                logging.error("Failed async backup for name: {} and mode: {} due to error: {}".format(name, mode, e))
                return None

        def backup(self, name, mode):
            try:
                backup_mode, stub = self.create_backup_stub(mode=mode)
                request = medusa_pb2.BackupRequest(name=name, mode=backup_mode)
                return stub.Backup(request)
            except grpc.RpcError as e:
                logging.error("Failed sync backup for name: {} and mode: {} due to error: {}".format(name, mode, e))
                return None

        def delete_backup(self, name):
            try:
                stub = medusa_pb2_grpc.MedusaStub(self.channel)
                request = medusa_pb2.DeleteBackupRequest(name=name)
                stub.DeleteBackup(request)
            except grpc.RpcError as e:
                logging.error("Failed to delete backup for name: {} due to error: {}".format(name, e))

        def get_backups(self):
            try:
                stub = medusa_pb2_grpc.MedusaStub(self.channel)
                request = medusa_pb2.GetBackupsRequest()
                response = stub.GetBackups(request)
                return response.backups
            except grpc.RpcError as e:
                logging.error("Failed to obtain list of backups due to error: {}".format(e))
                return None

        def get_backup_status(self, name):
            try:
                stub = medusa_pb2_grpc.MedusaStub(self.channel)
                request = medusa_pb2.BackupStatusRequest(backupName=name)
                resp = stub.BackupStatus(request)
                return resp.status
            except grpc.RpcError as e:
                logging.error("Failed to determine backup status for name: {} due to error: {}".format(name, e))
                return medusa_pb2.StatusType.UNKNOWN

        def backup_exists(self, name):
            try:
                backups = self.get_backups()
                for backup in list(backups):
                    if backup.backupName == name:
                        return True
                return False
            except grpc.RpcError as e:
                logging.error("Failed to determine if backup exists for backup name: {} due to error: {}".format(name, e))
                return False

        def purge_backups(self):
            try:
                stub = medusa_pb2_grpc.MedusaStub(self.channel)
                request = medusa_pb2.PurgeBackupsRequest()
                resp = stub.PurgeBackups(request)
                return resp
            except grpc.RpcError as e:
                logging.error("Failed to purge backups due to error: {}".format(e))
                return None

    if __name__ == '__main__':
        logging.basicConfig()
        client_stub = Client('localhost:50051')
        print("-------------- health_check --------------")
        client_stub.health_check()
        print("-------------- get_backups --------------")
        client_stub.get_backups()
        print("-------------- backing up : ",backupName)
        client_stub.backup(backupName,"full")
        print("-------------- back up complete : ",backupName)


  medusa.ini: |
    [cassandra]
    stop_cmd = /opt/cassandra/bin/cassandra stop
    start_cmd = /opt/cassandra/bin/cassandra start
    config_file = /etc/cassandra/cassandra.yaml
    cql_username = cassandra
    cql_password = cassandra
    nodetool_username = jmx_user
    nodetool_password_file_path = /secrets/jmxremote.password
    ;nodetool_host = cassandra-0.cassandra.default.svc.cluster.local
    nodetool_port = 7199
    nodetool_flags = "-h ::FFFF:127.0.0.1"
    sstableloader_bin = /opt/cassandra/bin/sstableloader
    nodetool_ssl = false
    check_running = nodetool -u jmx_user -pwf /secrets/jmxremote.password  version
    resolve_ip_addresses = True
    use_sudo = False

    [storage]
    storage_provider = google_storage
    region = europe-west1
    bucket_name = cassandrastsbackup
    key_file = /etc/medusa-secrets/medusa_gcp_key.json
    prefix = .cassandra
    max_backup_age = 5
    max_backup_count = 0
    transfer_max_bandwidth = 50MB/s
    concurrent_transfers = 1
    multi_part_upload_threshold = 104857600
    backup_grace_period_in_days = 10
    use_sudo_for_restore = False

    [monitoring]
    ;monitoring_provider = <Provider used for sending metrics. Currently either of "ffwd" or "local">


    [ssh]
    username = root
    key_file = /tmp/hostCerts/ssh_host_ed25519_key
    cert_file = /tmp/hostCerts/ssh_host_ed25519_key-cert.pub


    [checks]
    ;expected_rows = <Number of rows expected to be returned when the query runs. Not checked if not specified.>


    [logging]
    ; Controls file logging, disabled by default.
    enabled = 1
    file = medusa.log
    level = DEBUG
    ; Control the log output format
    format = [%(asctime)s] %(levelname)s: %(message)s
    ; Size over which log file will rotate
    maxBytes = 20000000
    ; How many log files to keep
    backupCount = 50


    [grpc]
    ; Set to true when running in grpc server mode.
    ; Allows to propagate the exceptions instead of exiting the program.
    enabled = 1

    [kubernetes]
    ; The following settings are only intended to be configured if Medusa is running in containers, preferably in Kubernetes.
    enabled = 1
    ;cassandra_url = <URL of the management API snapshot endpoint. For example: http://127.0.0.1:8080/api/v0/ops/node/snapshots>
    cassandra_url = http://127.0.0.1:8778/jolokia/
    ; Enables the use of the management API to create snapshots. Falls back to using Jolokia if not enabled.
    use_mgmt_api = 0
```

Apply the CronJob.yaml in the same namespace of the vanilla cassandra sts.
-------------------------------------------------------------------------
```
apiVersion: batch/v1
kind: CronJob
metadata: 
  name: medusa-grpc-backup
  namespace: <namespace of the sts file>
spec: 
  schedule: "35 0 * * 0-6"
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate: 
    spec: 
      template: 
        metadata: 
          name: medusa-grpc-backup
        spec:
          initContainers:
            - name: medusa-copy
              image: "alpine:3.15"
              resources:
                requests:
                  cpu: 20m
                  memory: 200Mi
              imagePullPolicy: IfNotPresent
              command: ["/bin/sh", "-c"]
              args:
                - cp ./data/scripts/* ./scripts;
                  apk add --update --no-cache --quiet curl coreutils; apk --quiet upgrade;
                  ./scripts/configure_k8s.sh;
                  sleep 2;
                  sh ./scripts/ascertain_cassandra_nodes.sh;
                  sleep 2;
                  sh ./scripts/copy_yaml.sh;
                  sleep 2;
                  echo "yamls copied from cass containers to medusa containers...";
                  sleep 10;
              volumeMounts:
                - name: cache-volume
                  mountPath: "/scripts"
                - name: data-scripts
                  mountPath: /data/scripts
          containers:
            - name: medusa-grpc-backup
              image: "python:3.8-slim-buster"
              resources:
                requests:
                  cpu: 20m
                  memory: 200Mi
              imagePullPolicy: IfNotPresent
              command: ["/bin/sh", "-c"]
              args:
                - apt update -qq && apt-get install -qq --no-install-recommends git curl > /dev/null;
                  rm -rf /var/lib/apt/lists/*;
                  git clone https://github.com/thelastpickle/cassandra-medusa.git;
                  cd cassandra-medusa;
                  pip install -r requirements-grpc.txt > /dev/null;
                  cd medusa/service/grpc;
                  python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. medusa.proto;
                  cp /data/scripts/client.py .;
                  sleep 2;
                  sh /data/scripts/configure_k8s.sh;
                  sleep 2;
                  sh /data/scripts/ascertain_cassandra_nodes.sh;
                  sleep 2;
                  sh /data/scripts/backup_with_medusa.sh;
                  sleep 2;
              volumeMounts:
                - name: data-scripts
                  mountPath: /data/scripts
          volumes:
            - name: data-scripts
              configMap: 
                defaultMode: 0755
                name: scripts
```

STS Modification Checklist for backup
-------------------------------------
- Check the manifests are in the same namespace and correct the manifests if its not.
- Run the Cronjob to ensure the backup in s3. See the logs of the pod. It should be steady.
- Check backups inside the bucket post running the CronJob.
- To ensure the k8ssadra migration works well, ensure there is a perfect backup. 
- Ensure atleast 2 or more sts backups using medusa on the s3 bucket. Check the contents of the s3 bucket as well to find the date and time references.
- Add random data via a client tool to schemas/tables/column data. Fire backups from the CronJob. Repeat this often.
- The next step is to restore test a backup.

Restore from s3 bucket to vanilla STS backup.
---------------------------------------
<b>Note</b>: Submit all the modifications to the sts yaml in one go.
- To restore from the backup ensure the backups taken above and you have backups visible in the s3 bucket.
- The cassandra pods should be labled the following via the sts yaml.
```
          labels:          
            cassandra: restore
```
- After the pods restart, please add the following snippet to the <b>initContainer</b> of the cassandra sts yaml and replace with the correct value for the env variable BACKUP_NAME. e.g. backup-06-03-2022-00-36-16
- Check the s3 backup bucket for the right reference. And chose which backup reference you like to restore test. 
- For the first time of a restore there is no RESTORE_KEY value so you can set any value to the variable that will be used again. This will be very important key value to fire the restore. 
- The medusa-cass-yaml container block below is used to extract the cassandra.yml as a template from a running statefulset cassandra pod. (its the vanilla statefulset cassandra)
- Then the template is set with the cassandra pod IP and ported to the medusa-restore container. 
- As and when you save the sts with these changes, the restore is fired from the s3 store against the BACKUP_NAME. The server-config is a shared folder so the /etc/cassandra/cassandra.yaml is passed from  medusa-cass-yaml container to medusa-restore container.
```
        - name: medusa-cass-yaml
          image: alpine:3.15
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - '-c'
          args:
            - >-
              cp ./data/scripts/* ./scripts;apk add --update --no-cache --quiet curl coreutils; apk --quiet upgrade;./scripts/configure_k8s.sh;
              sleep 2; kubectl get pod -l cassandra=restore --field-selector=status.phase=Running -n cassandra | awk -F" " '{print $1}' | grep -v NAME | head -1 > /tmp/firstPod;
              sleep 5; kubectl exec pod/"$(cat /tmp/firstPod | sed 's/\r$//')" -c cassandra -n cassandra -- tar cf - /etc/cassandra/cassandra.yaml | tar xvf - -C /tmp && cat /tmp/etc/cassandra/cassandra.yaml > /tmp/cassandra.yaml.template;
              sleep 4; cat /tmp/cassandra.yaml.template | sed "s/10\.[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}/$POD_IP/g" > /etc/cassandra/cassandra.yaml;
              sleep 2; cat /etc/cassandra/cassandra.yaml; sleep 10;
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
          resources:
            requests:
              cpu: 20m
              memory: 200Mi
          volumeMounts:
            - name: cache-volume
              mountPath: /scripts
            - name: data-scripts
              mountPath: /data/scripts
            - name: server-config
              mountPath: /etc/cassandra

        - name: medusa-restore
          image: docker.io/k8ssandra/medusa:0.11.3
          imagePullPolicy: IfNotPresent
          env:
            - name: MEDUSA_MODE
              value: RESTORE
            # Update this value and put some arbitrary string if you want to restore. 
            # The container saves this value and compares next restore so change it every time
            - name: RESTORE_KEY
              value: anyrandomkeyFirstTimeOnly
            # Update the value with the candidate you want to restore from s3 bucket. 
            # Ensure the bucket has this folder in all the cass node folders.  
            - name: BACKUP_NAME
              value: backup-06-03-2022-00-36-16
          resources: {}
          volumeMounts:
            - name: cassandra-medusa
              mountPath: /etc/medusa
            - name: server-config
              mountPath: /etc/cassandra
            - name: cassandra-data
              mountPath: /var/lib/cassandra
            - name: podinfo
              mountPath: /etc/podinfo
            - name: google-storage-s3-json
              mountPath: /etc/medusa-secrets
            - name: kube-api-access-d9chp
              readOnly: true
              mountPath: /var/run/secrets/kubernetes.io/serviceaccount
```

Checklist
---------
- The moment the cassandra sts yaml is saved a restore is fired. You would be witnessing a rolling update of pods.
- Check if all the logs of the medusa-restore initContainer looks good. If all is good, use the Cassandra client tool to check the schema/table values. 
- If all is good, next step is to proceed with the k8ssandra migration as mentioned in the link https://k8ssandra.io/blog/tutorials/how-to/how-to-migrate-an-existing-cluster-to-k8ssandra-operator-without-any-downtime/. Ensure to check the cluster names of the source and the destination. 


Migration to k8ssandra from sts.
---------------------------------
This procedure is done post the backup and restore test on plain STS cassandra.

Steps:
------
- In the cassandra sts cluster : run `nodetool status` check cassandra status.
```
$ nodetool status
Datacenter: DC1-K8Demo
=====================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load      Tokens  Owns (effective)  Host ID                               Rack
UN  172.31.4.217   10.2 GiB  16      100.0%            9a9b5e8f-c0c2-404d-95e1-372880e02c43  us-west-2c
UN  172.31.38.15   10.2 GiB  16      100.0%            1e6a9077-bb47-4584-83d5-8bed63512fd8  us-west-2b
UN  172.31.22.153  10.2 GiB  16      100.0%            d6488a81-be1c-4b07-9145-2aa32675282a  us-west-2a
```
- In Cassandra sts cluster go to the casandra client tool SQL IDE or the cqlsh cli of cassandra container. 
- Alter these keyspaces to use NetworkTopologyStrategy `system_auth,system_distributed,system_traces and other non-system/user keyspaces`. 
- Ensure the DC name as used from he `nodetool status` output.
```
cqlsh> ALTER KEYSPACE <keyspace_name> WITH replication = {'class': 'NetworkTopologyStrategy', 'DC1-K8Demo': 3};
```
- We need to make a new k8ssandra cluster for the cass-operator with the below manifest `values.yaml`
- It is optional to keep the same cassandra cluster name as source cluster as the cassandra clients using the new cluster will check for some cluster variables used on the cassandra client-end for eg. CASSANDRA_CLUSTER_NAME or similar. ( please check client app variables that connect cassandra cluster). We have used the same cluster name as source cluster here so that our cassandra client apps donot complain. We have used a different DC name k8s-1 to identify the k8ssandra DC.
- Update the IP of cassandra-0
```
# values.yaml
cassandra:
  # version: "4.0.0"
  version "3.11.12"
  clusterName: "cluster"
  allowMultipleNodesPerWorker: false
  additionalSeeds:
  # it is the cassandra-0 node IP from the cassandra STS cluster (source cluster). Please update this value. Possibly with internal or external IP.
  - 172.31.4.217
  # you can also provide domain name cassandra-0.cassandra.default.svc.cluster.local. It should be a service with a valid port
  heap:
   size: 31g
  gc:
    g1:
      enabled: true
      setUpdatingPauseTimePercent: 5
      maxGcPauseMillis: 300
  resources:
    requests:
      memory: "59Gi"
      cpu: "7000m"
    limits:
      memory: "60Gi"
  datacenters:
  - name: k8s-1
    size: 3
    racks:
    - name: r1
      affinityLabels:
        topology.kubernetes.io/zone: europe-west-1a
    - name: r2
      affinityLabels:
        topology.kubernetes.io/zone: europe-west-1b
    - name: r3
      affinityLabels:
        topology.kubernetes.io/zone: europe-west-1c
  ingress:
    enabled: false
  cassandraLibDirVolume:
    storageClass: standard-rwo
    size: 100Gi
stargate:
  enabled: false
# add medusa config later when migration is a success.
medusa:
  enabled: false
reaper-operator:
  enabled: false
kube-prometheus-stack:
  enabled: false
reaper:
  enabled: false
```
Make the new k8ssandra cluster
------------------------------
```
helm install k8ssandra charts/k8ssandra -n cassandra --create-namespace -f values.yaml
```
- do a `nodetool status` after 10 mins in the source cluster cassandra node. Warch the owns( its 0%)
```
Datacenter: k8s-1
=================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens  Owns (effective)  Host ID                               Rack
UN  10.0.3.10      78.16 KiB  16      0.0%              c63b9b16-24fe-4232-b146-b7c2f450fcc6  europe-west-1a
UN  10.0.2.66      69.14 KiB  16      0.0%              b1409a2e-cba1-482f-9ea6-c895bf296cd9  europe-west-1b
UN  10.0.1.77      69.13 KiB  16      0.0%              78c53702-7a47-4629-a7bd-db41b1705bb8  europe-west-1c
Datacenter: DC1-K8Demo
=====================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens  Owns (effective)  Host ID                               Rack
UN  172.31.4.217   10.2 GiB   16      100.0%            9a9b5e8f-c0c2-404d-95e1-372880e02c43  europe-west-1a
UN  172.31.38.15   10.2 GiB   16      100.0%            1e6a9077-bb47-4584-83d5-8bed63512fd8  europe-west-1b
UN  172.31.22.153  10.2 GiB   16      100.0%            d6488a81-be1c-4b07-9145-2aa32675282a  europe-west-1c
```
- Alter the keyspaces in the new k8ssandra cluster.
- system_auth,system_distributed,system_traces and other non-system/user keyspaces. 
```
cqlsh> ALTER KEYSPACE <keyspace_name> WITH replication = {'class': 'NetworkTopologyStrategy', 'DC1-K8Demo': '3', 'k8s-1': '3'};
```
Run rebuild on new cluster
```
   # kubectl exec -it pod/k8s-1-r1-sts-0 -c cassandra -n k8ssandra -- nodetool rebuild DC1-K8Demo
   # kubectl exec -it pod/k8s-1-r1-sts-1 -c cassandra -n k8ssandra -- nodetool rebuild DC1-K8Demo
   # kubectl exec -it pod/k8s-1-r1-sts-2 -c cassandra -n k8ssandra -- nodetool rebuild DC1-K8Demo
```
- final output. Watch the owns ( 100%)
```
Datacenter: k8s-1
=================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens  Owns (effective)  Host ID                               Rack
UN  10.0.3.10      78.16 KiB  16      100.0%              c63b9b16-24fe-4232-b146-b7c2f450fcc6  europe-west-1a
UN  10.0.2.66      69.14 KiB  16      100.0%              b1409a2e-cba1-482f-9ea6-c895bf296cd9  europe-west-1b
UN  10.0.1.77      69.13 KiB  16      100.0%              78c53702-7a47-4629-a7bd-db41b1705bb8  europe-west-1c
Datacenter: DC1-K8Demo
=====================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens  Owns (effective)  Host ID                               Rack
UN  172.31.4.217   10.32 GiB  16      100.0%            9a9b5e8f-c0c2-404d-95e1-372880e02c43  europe-west-1a
UN  172.31.38.15   10.32 GiB  16      100.0%            1e6a9077-bb47-4584-83d5-8bed63512fd8  europe-west-1b
UN  172.31.22.153  10.32 GiB  16      100.0%            d6488a81-be1c-4b07-9145-2aa32675282a  europe-west-1c
```
- check the data is present in the new cluster.
- decommission the old cluster. These are commands if the cluster is in the default namespace
```
   # kubectl exec -it pod/cassandra-0 -c cassandra -n default -- nodetool decommission
   # kubectl exec -it pod/cassandra-1 -c cassandra -n default -- nodetool decommission
   # kubectl exec -it pod/cassandra-2 -c cassandra -n default -- nodetool decommission
```