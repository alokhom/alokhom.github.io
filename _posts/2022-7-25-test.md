Migrate from Cassandra in a vanilla STS to K8ssandra
----------------------------------------------------

Most users of k8ssandra have either started a new Cassandra cluster or have migrated from an existing Cassandra cluster. 

In containerized Cassandra clusters like the vanilla STS Cassandra cluster, there is no earlier backups made perhaps (besides disc snapsots) or rather Admins are stuck in a assumption that the k8ssandra migration is straight forward and flawless. 

The Risk is what if the migration to k8ssandra is not a success ? And therefore what is the fallback / migrating plan from a STS cluster point of view. It involves risk of losing data. 

Can we have a backup/restore test on the vanilla STS cluster before we proceed with migration to k8ssandra cluster ?

There is no documented resources on the internet for backup and restore of vanilla STS cluster and an attempt was made with this solution and it kind of works. 
Repo: https://github.com/alokhom/cassandra-statefulset-to-k8ssandra-migration 
Request feedback and continuous improvement. 

To simulate the situation let us make a STS cluster on gke. 
- We have 3 pod vanilla STS cluster with 10 GB disc each.
- K8Demo is the cluster name
- We assume some clients old and new are using typical CASSANDRA varaibles to connect to the cluster via a common configMap that can also be used as a env variable on this manifest.
```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cassandra
  labels:
    app: cassandra
spec:
  serviceName: cassandra
  replicas: 3
  selector:
    matchLabels:
      app: cassandra
  template:
    metadata:
      labels:
        app: cassandra
    spec:
      terminationGracePeriodSeconds: 1800
      containers:
      - name: cassandra
        image: gcr.io/google-samples/cassandra:v13
        imagePullPolicy: Always
        ports:
        - containerPort: 7000
          name: intra-node
        - containerPort: 7001
          name: tls-intra-node
        - containerPort: 7199
          name: jmx
        - containerPort: 9042
          name: cql
        resources:
          limits:
            cpu: "500m"
            memory: 1Gi
          requests:
            cpu: "500m"
            memory: 1Gi
        securityContext:
          capabilities:
            add:
              - IPC_LOCK
        lifecycle:
          preStop:
            exec:
              command: 
              - /bin/sh
              - -c
              - nodetool drain
        env:
          - name: MAX_HEAP_SIZE
            value: 512M
          - name: HEAP_NEWSIZE
            value: 100M
          - name: CASSANDRA_SEEDS
            value: "cassandra-0.cassandra.default.svc.cluster.local"
          - name: CASSANDRA_CLUSTER_NAME
            value: "K8Demo"
          - name: CASSANDRA_DC
            value: "DC1-K8Demo"
          - name: CASSANDRA_RACK
            value: "Rack1-K8Demo"
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - /ready-probe.sh
          initialDelaySeconds: 15
          timeoutSeconds: 5
        # These volume mounts are persistent. They are like inline claims,
        # but not exactly because the names need to match exactly one of
        # the stateful pod volumes.
        volumeMounts:
        - name: cassandra-data
          mountPath: /cassandra_data
  volumeClaimTemplates:
  - metadata:
      name: cassandra-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: standard-rwo
      resources:
        requests:
          storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: cassandra
  name: cassandra
spec:
  ports:
  - port: 9042
  selector:
    app: cassandra
```

- In this yaml there is no scope of automated backup usign medusa.  ( Disc snapshots in google cloud are possible and has their own risks involved. ) 
