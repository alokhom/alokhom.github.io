Migrate from Cassandra in a vanilla STS to K8ssandra
----------------------------------------------------

Most users of k8ssandra have either started a new Cassandra cluster or have migrated from an existing Cassandra cluster. 

In containerized Cassandra clusters like the vanilla STS Cassandra cluster, there is no earlier backups made perhaps (besides disc snapsots) or rather Admins are stuck in a assumption that the k8ssandra migration is straight forward and flawless. 

The Risk is what if the migration to k8ssandra is not a success ? And therefore what is the fallback / migrating plan from a STS cluster point of view. It involves risk of losing data. 

Can we have a backup/restore test on the vanilla STS cluster before we proceed with migration to k8ssandra cluster ?

There is no documented resources on the internet for backup and restore of vanilla STS cluster and an attempt was made with this solution and it kind of works. 
Repo: https://github.com/alokhom/cassandra-statefulset-to-k8ssandra-migration 
Request feedback and continuous improvement. 

To simulate the situation let us make a STS cluster on gke. 
- We have 3 pod vanilla STS cluster with 10 GB disc each.
- K8Demo is the cluster name
- We assume some clients old and new are using typical CASSANDRA varaibles to connect to the cluster via a common configMap that can also be used as a env variable on this manifest.
```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cassandra
  labels:
    app: cassandra
spec:
  serviceName: cassandra
  replicas: 3
  selector:
    matchLabels:
      app: cassandra
  template:
    metadata:
      labels:
        app: cassandra
    spec:
      terminationGracePeriodSeconds: 1800
      containers:
      - name: cassandra
        image: gcr.io/google-samples/cassandra:v13
        imagePullPolicy: Always
        ports:
        - containerPort: 7000
          name: intra-node
        - containerPort: 7001
          name: tls-intra-node
        - containerPort: 7199
          name: jmx
        - containerPort: 9042
          name: cql
        resources:
          limits:
            cpu: "500m"
            memory: 1Gi
          requests:
            cpu: "500m"
            memory: 1Gi
        securityContext:
          capabilities:
            add:
              - IPC_LOCK
        lifecycle:
          preStop:
            exec:
              command: 
              - /bin/sh
              - -c
              - nodetool drain
        env:
          - name: MAX_HEAP_SIZE
            value: 512M
          - name: HEAP_NEWSIZE
            value: 100M
          - name: CASSANDRA_SEEDS
            value: "cassandra-0.cassandra.default.svc.cluster.local"
          - name: CASSANDRA_CLUSTER_NAME
            value: "K8Demo"
          - name: CASSANDRA_DC
            value: "DC1-K8Demo"
          - name: CASSANDRA_RACK
            value: "Rack1-K8Demo"
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - /ready-probe.sh
          initialDelaySeconds: 15
          timeoutSeconds: 5
        # These volume mounts are persistent. They are like inline claims,
        # but not exactly because the names need to match exactly one of
        # the stateful pod volumes.
        volumeMounts:
        - name: cassandra-data
          mountPath: /cassandra_data
  volumeClaimTemplates:
  - metadata:
      name: cassandra-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: standard-rwo
      resources:
        requests:
          storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: cassandra
  name: cassandra
spec:
  ports:
  - port: 9042
  selector:
    app: cassandra
```

- In this yaml there is no scope of automated backup usign medusa, a popular cassandra backup solution.  ( Disc snapshots in google cloud are possible and has their own risks involved. ) 

Using medusa over a vanilla sts cassandra cluster - no downtime.
--------------------------------------------------------------
- Read more about Medusa here. https://github.com/thelastpickle/cassandra-medusa
- Medusa is also used in k8ssandra operator.
- Medusa is also offered as a Docker image. 
- You will need a s3 storage to backup and restore your vanilla sts cluster and configure your s3 bucket. For this demo we have selected the gcs setup from https://github.com/thelastpickle/cassandra-medusa/tree/master/docs. You could select your own s3 preference.
- How to use medusa in the sts vanilla cluster.

Backup. 
-------
- Trigger daily backups using medusa via a CronJob.
- To trigger this we will do the following steps.
- We will need to define volumes that refer to a configMap and also google-storage-s3-json secret. The jolokia-share helps provide the jar from the initContainer to the medusa container.
```
 volumes:
   - name: jolokia-share
     emptyDir: {}
   - name: server-config
     emptyDir: {}
   - name: cassandra-medusa
     configMap:
       name: scripts
       items:
         - key: medusa.ini
           path: medusa.ini
   - name: google-storage-s3-json
     secret:
       secretName: google-storage-s3-json
       defaultMode: 420
   - name: medusa-scripts
     configMap: 
       defaultMode: 0755
       name: scripts
       items:
         - key: get_cassandra_node_names.sh
           path: get_cassandra_node_names.sh
```
- We will use a jolokia jvm agent in which we add the initContainer and volumeMount on the cassandra sts yaml.
```
 initContainers:
   - name: install-jolokia-jvm-agent
     image: busybox
     command:
       - sh
       - '-c'
       - >-
         wget -O /usr/share/java/jolokia-jvm-1.6.2-agent.jar
         http://search.maven.org/remotecontent?filepath=org/jolokia/jolokia-jvm/1.6.2/jolokia-jvm-1.6.2-agent.jar
     resources: {}
     volumeMounts:
       - name: jolokia-share
         mountPath: /usr/share/java
```


